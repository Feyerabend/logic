
# Logic


## A personal experience

From the east, a bright light from the sun shines on the bright yellow facade
of the university building. It is Monday morning, July 2, 1984, and *the Second
International Conference in Logic Programming* is about to begin in Uppsala ...

[description of something in the conference from proceedings??] The conference did not go unnoticed. Computer Sweden, the then Swedish news agency in newspaper format for the computer area, made a report. Even Sweden's Television had a feature in the news program Rapport. Even at this time, it was quite unusual for a conference to be noticed by the national media.

What was I expecting? When I started my studies at the university in the
spring of 1982, I had an idea that philosophy could be combined with the
science of data and programming in some way. Ignorant as I was, however,
I had no idea how they were to be united. The conference, on the other hand,
clearly declared in the title that logic, which had its historical connection
to philosophy, and programming to the data area were part of a kind of fusion:
logic programming. After starting with theoretical philosophy, I found out
that I had also entered systems science in Stockholm in the first spring.
Well, maybe it can wait, I thought. Not sure if my knowledge was sufficient,
my fear, my shyness or the uncertainty of whether I was smart enough, stopped
me from trying to read the data on a three-year line.

With some surprise I found no one from the philosophy department present at
the conference. On the other hand, there were several people from the department
under the professor of computer science Sten-Åke Tärnlund. Several promising
students came a few years later to go over to Japan with a substantial salary
increase to participate in the new construction of the fifth generation computer
technology project. The Japanese state invested huge sums through its Ministry
of Trade and Industry (MITI) in the construction of knowledge systems, expert
systems, artificial intelligence (AI) from a more logical perspective as a base,
large databases rather than file systems, new parallel supercomputers with a
symbolic basis rather than arithmetic, as well as logic programming. In the
future, the computers would converse with us, reason like us, translate languages,
or interpret images. At this time, Japan was seen as a miracle of success. It was
cheap electronics, cheap cars that washed over the West but were no longer seen
as copies but often of better quality than what Europe or the US could produce.
Some researchers in the United States feared that Japan did not stop at cars or
electronic watches, but also took over computer development and thus future
industrial and research development. So it was speculated that "[...] the next
generation of computers think, reason, and speak – in Japanese.".[^1]
Artificial intelligence (AI) researcher Edward A. Feigenbaum and journalist
Pamela McCorduck reacted fiercely, with the American response to the Japanese
challenge, they write in their book *The fifth generation: artificial intelligence
and Japan's computer challenge to the world*: “In the end, we have no choice. We
can decide when we shall participate, not if.”[^2] In hindsight, however, the entire
ten-year Japanese project turns out to be a phenomenal failure. Fifty billion yen
costs hope with meager results.[^3] There are probably many different explanations
for why it is a failure. One reason that has been pointed to is that the main
exponential development of computers and processors better known under the name
'Moore's Law', also constantly drove down the price of the hardware.[^4] The project
with the fifth generation aimed, among other things, to develop expensive special
hardware with parallel working processors which in the long run could not be
justified or compete with the main development.[^5] Whatever the reasons, the hot
streaks of optimism cooled over time. The answer to the Japanese project was the
US's competing effort 'Strategic Computing Initiative' backed by the military's
department for technical development DARPA (Defense Advanced Research Projects
Agency). But the SCI venture met the same fate as the Japanese one and shrank
drastically financially towards the end of the 80s.[^6]

[^1]: From the back cover text of the paperback edition of Edward A. Feigenbaum,
& Pamela McCorduck, The fifth generation: artificial intelligence and Japan's
computer challenge to the world, Rev. & upd. ed., Pan Books, London, 1984.

[^2]: Ibid, p. 290.

[^3]: Wikipedia, "Fifth generation computer,"
https://en.wikipedia.org/w/index.php?title=Fifth_generation_computer&oldid=891491126
(retrieved 2019-04-09).

[^4]: Wikipedia, “Moore's law,”
https://en.wikipedia.org/w/index.php?title=Moore%27s_law&oldid=891671193
(retrieved 2019-04-11)

[^5]: Wikipedia, “Lisp Machines,”
https://en.wikipedia.org/windex.php?title=Lisp_Machines&oldid=871717316
(retrieved 2019-04-13)

[^6]: Wikipedia, “AI winter,”
https://en.wikipedia.org/w/index.php?title=AI_winter&oldid=891874138
(retrieved 2019-04-11)


Maybe my professor of theoretical philosophy Stig Kanger had already foreseen
the problems, or maybe he wasn't interested. Kanger would not call himself an
analytical philosopher, but he was clearly a very great logician. He was warm,
kind but a little tight-lipped. As a matter of principle, all his work had been
in the formal logical arena. He was very clear about what he liked and
didn't like. I had only been a student for two years in 1984 and cautiously
walked up the stairs to his room to ask how I should approach the D paper.
I knocked, still convinced that there should be some connection between the
technicalities of data and the philosophy, I ventured to suggest that I read
*Logic for Problem Solving* by Robert Kowalski. In his textbook, Kowalski
shows the connection of logic to programming through such things as a
certain formulation of logical propositions in Horn clauses, inferences,
matching between propositions or the interpretation of negation as failure
of proof. Everything possible seemed to be practically programmable in the
Prolog programming language. Kanger retorted that he had never heard of the
author and headed for the bookshelf to quickly pull out a book that I should
read instead. He suggested Rudolf Carnap's Meaning and Necessity. Somewhat
disappointed, however, I trusted Kanger's recommendations completely.
In retrospect, I can add that he was also right. I read through Carnap,
who was a phenomenal thinker, but couldn't say I had anything to add.
Later, however, I instead came to write about the medieval philosopher
William Occam and the assertion (assertio), also at the suggestion of Kanger.
Although I got the nice offer to start a PhD with Professor Jaakko Hintikka
at Stanford after Kanger seemed to appreciate my efforts, my interests had
now started to lean more towards history and the history of philosophy.
Partly as an effect of reading texts by Occam. But after studying more of
the history of philosophy, the history of ideas and learning, as well as the
cultural and social life of antiquity during the latter part of the 1980s,
my interest in practical programming and philosophy had not completely
stopped. I went to UPMAIL seminars at the university where many different
kinds of logic and programming were discussed, sometimes hosted by philosophers
such as the professor of theoretical philosophy in Stockholm Dag Prawitz,
but mostly by computer scientists who organized the seminars. Object-orientation,
a way of coding programs, got a boost in the 80s through the programming language
Smalltalk, where unfortunately the books were so expensive that I could only visit the Akademibokhandeln in Stockholm to browse the copies. However, publications in
artificial intelligence were sometimes affordable even for my student wallet,
especially if they were out of date and a new edition replaced an obsolete old one.
Of course, my understanding was mostly in the direction within AI that leaned towards
logic as a source of inspiration. Much was theory about search trees, syntactic
analysis in grammar or semantic networks. But little was also about programming,
which I already discovered during junior high school. In addition to the declarative
Prolog, functional LISP was often used as a programming language. Without access to
a computer, without access to programming languages, outside the technical
institutions, I got to 'dry swim the code' through the books.

After various kinds of attempts during the 1980s to understand, convince, argue,
discuss with everyone I could even if I had little faith they were interested but
all with setbacks, I was lost and doubted whether combination really existed or
could exist. Was the combination of philosophy and computers that I imagined just
a wishful dream? I also grew increasingly tired of higher seminars which seemed to
me to be getting nowhere. I was getting tired of university courses. During the
spring of 1992, however, the then docent in theoretical philosophy Sten Lindström
had a small course in AI. He had introduced me to model theory and Kripke semantics
in another modal logic course exactly ten years earlier. It was possibly the first
ever AI course at the institution, although I saw in the possible courses under
Kanger there was one in automata theory, which in principle could be close.
Lindström went through common problems such as consciousness, different ideas
about how AI is defined and what it is, but also arguments from John Searle
questioning whether the computer or AI really understands or has thoughts.
But what was particularly interesting to me at the time was that Lindström
was the very last to take up a direction of what was then called connectionism.
Connectionism is contrasted with classical AI. Lindström writes in the lecture
notes: "Classical AI wants to construct computers that can represent their
environment and that can reason about this environment using formal logical rules."
This is what they call good old fashion AI (after the English concept of GOFAI or
“good old fashion AI”). This takes place through the implementation of specified
functions, which become algorithms in the computer. On the other hand, connectionism
rather tries to construct computers that mimic the brain in neural networks:
"While classical AI sees intelligence mainly as symbolic thinking, connectionists
emphasize learning and adaptive behavior." Lindström then describes examples of
how an artificial neural network is structured, but limits the networks to a
single layer and a special case of backpropagation in the delta rule. But we
shall not now immerse ourselves in how these networks work now. We come up
again how important it was with this 'parallel' path to give perspective on AI.

Slowly over the course of a few years in the early 90s, I left university and
the academic sphere. It took time for me to understand the situation myself.
Probably it was combinations of disillusionment, increasingly lacking self-esteem,
bordering on depression, calling out without hearing an answer. My attitude towards
what I could do or not do changed quite drastically with hindsight. The transition
was not easy or that I even had the outline of a changed life plan. As with many
who experience it, the change can only be there suddenly. However, the development
of the computer came to keep up in the future, but now in a different way, which
we will return to later.
